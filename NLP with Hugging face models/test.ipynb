{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub==0.2.1\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface_hub==0.2.1) (3.13.1)\n",
      "Requirement already satisfied: requests in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface_hub==0.2.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface_hub==0.2.1) (4.65.0)\n",
      "Requirement already satisfied: pyyaml in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface_hub==0.2.1) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface_hub==0.2.1) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface_hub==0.2.1) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.2.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.2.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.2.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.2.1) (2023.11.17)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.12.0 requires huggingface-hub<1.0.0,>=0.11.0, but you have huggingface-hub 0.2.1 which is incompatible.\n",
      "transformers 4.32.1 requires huggingface-hub<1.0,>=0.15.1, but you have huggingface-hub 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.15.0\n",
      "  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: filelock in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (0.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (2.31.0)\n",
      "Collecting sacremoses (from transformers==4.15.0)\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.15.0)\n",
      "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from transformers==4.15.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->transformers==4.15.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->transformers==4.15.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->transformers==4.15.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (from requests->transformers==4.15.0) (2023.11.17)\n",
      "Collecting click (from sacremoses->transformers==4.15.0)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from sacremoses->transformers==4.15.0)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-11.1-arm64-cpython-38/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CommitOperationAdd' from 'huggingface_hub' (/Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m semantic_search\n",
      "File \u001b[0;32m~/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[39m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     logging,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     46\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdependency_versions_table\u001b[39;00m \u001b[39mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversions\u001b[39;00m \u001b[39mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[39m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[39m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtqdm\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpyyaml\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages/transformers/utils/__init__.py:61\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdoc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     25\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     ContextManagers,\n\u001b[1;32m     33\u001b[0m     ExplicitEnum,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     63\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m     64\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m     65\u001b[0m     HUGGINGFACE_CO_PREFIX,\n\u001b[1;32m     66\u001b[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[1;32m     67\u001b[0m     PYTORCH_PRETRAINED_BERT_CACHE,\n\u001b[1;32m     68\u001b[0m     PYTORCH_TRANSFORMERS_CACHE,\n\u001b[1;32m     69\u001b[0m     S3_BUCKET_PREFIX,\n\u001b[1;32m     70\u001b[0m     TRANSFORMERS_CACHE,\n\u001b[1;32m     71\u001b[0m     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n\u001b[1;32m     72\u001b[0m     EntryNotFoundError,\n\u001b[1;32m     73\u001b[0m     PushInProgress,\n\u001b[1;32m     74\u001b[0m     PushToHubMixin,\n\u001b[1;32m     75\u001b[0m     RepositoryNotFoundError,\n\u001b[1;32m     76\u001b[0m     RevisionNotFoundError,\n\u001b[1;32m     77\u001b[0m     cached_file,\n\u001b[1;32m     78\u001b[0m     default_cache_path,\n\u001b[1;32m     79\u001b[0m     define_sagemaker_information,\n\u001b[1;32m     80\u001b[0m     download_url,\n\u001b[1;32m     81\u001b[0m     extract_commit_hash,\n\u001b[1;32m     82\u001b[0m     get_cached_models,\n\u001b[1;32m     83\u001b[0m     get_file_from_repo,\n\u001b[1;32m     84\u001b[0m     has_file,\n\u001b[1;32m     85\u001b[0m     http_user_agent,\n\u001b[1;32m     86\u001b[0m     is_offline_mode,\n\u001b[1;32m     87\u001b[0m     is_remote_url,\n\u001b[1;32m     88\u001b[0m     move_cache,\n\u001b[1;32m     89\u001b[0m     send_example_telemetry,\n\u001b[1;32m     90\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     93\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m     94\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m     torch_only_method,\n\u001b[1;32m    181\u001b[0m )\n\u001b[1;32m    184\u001b[0m WEIGHTS_NAME \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpytorch_model.bin\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages/transformers/utils/hub.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     CommitOperationAdd,\n\u001b[1;32m     35\u001b[0m     create_commit,\n\u001b[1;32m     36\u001b[0m     create_repo,\n\u001b[1;32m     37\u001b[0m     get_hf_file_metadata,\n\u001b[1;32m     38\u001b[0m     hf_hub_download,\n\u001b[1;32m     39\u001b[0m     hf_hub_url,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile_download\u001b[39;00m \u001b[39mimport\u001b[39;00m REGEX_COMMIT_HASH, http_get\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     EntryNotFoundError,\n\u001b[1;32m     44\u001b[0m     GatedRepoError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     hf_raise_for_status,\n\u001b[1;32m     50\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CommitOperationAdd' from 'huggingface_hub' (/Users/prasannasundar/Projects/Using OpenAI and Langchain/NLP and AI /.conda/lib/python3.8/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import the other required packages and modules.\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "\n",
    "# From the IPython.display package, import display and Markdown\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
